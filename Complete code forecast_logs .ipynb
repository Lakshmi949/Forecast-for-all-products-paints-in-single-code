{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense,RepeatVector\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from pandas.io.json import json_normalize\n",
    "from elasticsearch_dsl import Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "connection = pymysql.connect(host='mysql.cw34ebrphxxg.eu-central-1.rds.amazonaws.com',\n",
    "                         user='admin',\n",
    "                         password='jazeera123',\n",
    "                         db='TEST_dB')\n",
    "\n",
    "cursor=connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic = Elasticsearch(\"https://es.jazeerapaints.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unique_items(Region,Channel):\n",
    "    \n",
    "    list=[]\n",
    "    df=elastic.search(index=\"jaz*\", body={\n",
    "      \"query\": {\n",
    "        \"bool\": {\n",
    "          \"must\": [\n",
    "            {\n",
    "              \"match\": {\n",
    "                \"region\": Region\n",
    "              }\n",
    "            },\n",
    "            {\n",
    "              \"match\": {\n",
    "                \"saleschannel\": Channel\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "                          \"aggs\" : {\n",
    "                              \"products\" : {\n",
    "                                  \"terms\" : {\n",
    "                                      \"field\" : \"itemid.keyword\",\n",
    "                                      \"size\" : 9000\n",
    "                                   }\n",
    "                              }\n",
    "                          }\n",
    "                          })\n",
    "    \n",
    "    for aggregation in df[\"aggregations\"][\"products\"][\"buckets\"]:\n",
    "        list.append(aggregation['key'])\n",
    "        \n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_colours(Region,Channel,itemid):\n",
    "    list=[]\n",
    "    df=elastic.search(index=\"jaz*\", body={\n",
    "      \"query\": {\n",
    "        \"bool\": {\n",
    "          \"must\": [\n",
    "            {\n",
    "              \"match\": {\n",
    "                \"region\": Region\n",
    "              }\n",
    "            },\n",
    "            {\n",
    "              \"match\": {\n",
    "                \"saleschannel\": Channel\n",
    "              }\n",
    "            },\n",
    "             {\n",
    "              \"match\": {\n",
    "                \"itemid\": itemid\n",
    "              }\n",
    "            } \n",
    "              \n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "                          \"aggs\" : {\n",
    "                              \"colour\" : {\n",
    "                                  \"terms\" : {\n",
    "                                      \"field\" : \"colour.keyword\",\n",
    "                                      \"size\" : 9000\n",
    "                                   }\n",
    "                              }\n",
    "                          }\n",
    "                          })\n",
    "    \n",
    "    for aggregation in df[\"aggregations\"][\"colour\"][\"buckets\"]:\n",
    "        list.append(aggregation['key'])\n",
    "        \n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_sizes(Region,Channel,itemid,colour):\n",
    "    list=[]\n",
    "    df=elastic.search(index=\"jaz*\", body={\n",
    "      \"query\": {\n",
    "        \"bool\": {\n",
    "          \"must\": [\n",
    "            {\n",
    "              \"match\": {\n",
    "                \"region\": Region\n",
    "              }\n",
    "            },\n",
    "            {\n",
    "              \"match\": {\n",
    "                \"saleschannel\": Channel\n",
    "              }\n",
    "            },\n",
    "             {\n",
    "              \"match\": {\n",
    "                \"itemid\": itemid\n",
    "              }\n",
    "            },\n",
    "              {\n",
    "              \"match\": {\n",
    "                \"colour\": colour\n",
    "              }\n",
    "            }\n",
    "              \n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "                          \"aggs\" : {\n",
    "                              \"size\" : {\n",
    "                                  \"terms\" : {\n",
    "                                      \"field\" : \"size.keyword\",\n",
    "                                      \"size\" : 9000\n",
    "                                   }\n",
    "                              }\n",
    "                          }\n",
    "                          })\n",
    "    \n",
    "    for aggregation in df[\"aggregations\"][\"size\"][\"buckets\"]:\n",
    "        list.append(aggregation['key'])\n",
    "        \n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensions(Region,Channel,itemid,colour,size):\n",
    "    \n",
    "    df11 = elastic.search(index=\"jaz*\", body={\n",
    "      \"query\": {\n",
    "        \"bool\": {\n",
    "          \"must\": [\n",
    "            {\n",
    "              \"match\": {\n",
    "                \"region\": Region\n",
    "              }\n",
    "            },\n",
    "            {\n",
    "              \"match\": {\n",
    "                \"saleschannel\": Channel\n",
    "              }\n",
    "            },{\n",
    "                  \"match\": {\n",
    "                    \"itemid\": itemid\n",
    "                  }\n",
    "                },\n",
    "                  {\n",
    "                  \"match\": {\n",
    "                    \"colour\": colour\n",
    "                  }\n",
    "                },\n",
    "                  {\n",
    "                  \"match\": {\n",
    "                    \"size\": size\n",
    "                  }\n",
    "                }\n",
    "          ]\n",
    "        }\n",
    "      },     \"aggs\": {\n",
    "        \"amount_per_day\": {\n",
    "          \"date_histogram\": {\n",
    "            \"field\": \"invoicedate\",\n",
    "            \"interval\": \"day\",\n",
    "            \"format\" : \"yyyy-MM-dd\"\n",
    "          },\n",
    "          \"aggs\": {\n",
    "            \"total_amount\": {\n",
    "              \"sum\": {\n",
    "                \"field\": \"actualvalue\"\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "\n",
    "    },size=9000)\n",
    "    \n",
    "    list_dates=[]\n",
    "    list_values=[]\n",
    "    for aggregation in df11[\"aggregations\"][\"amount_per_day\"][\"buckets\"]:\n",
    "\n",
    "        list_dates.append(aggregation['key_as_string'])\n",
    "        list_values.append(aggregation['total_amount']['value'])\n",
    "    df1 = pd.DataFrame({'INVOICEDATE' : list_dates,'LINEAMOUNT' : list_values})\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forecast1(Region,channel,Forecast_days):\n",
    "    count_i=0\n",
    "    ID=0\n",
    "    elastic = Elasticsearch(\"https://es.jazeerapaints.com/\")\n",
    "    u_items = unique_items(Region,channel)\n",
    "    check='HSN'\n",
    "    Remove=[u_items.remove(x) for x in u_items if x.lower().startswith(check.lower())]\n",
    "    count_items=len(u_items)\n",
    "    doc1 = {'Total no. of items':count_items,\n",
    "            'timestamp': datetime.now()}\n",
    "    ID+=1\n",
    "    res = elastic.index(index=\"log-index\", id=ID, body=doc1)\n",
    "    elastic.indices.refresh(index=\"log-index\")\n",
    "    print(\"Total no. of items-\",count_items) \n",
    "    for i in u_items:\n",
    "        count_i+=1    \n",
    "        u_colors = unique_colours(Region,channel,i)\n",
    "        count_colours=len(u_colors)\n",
    "        doc2 = {\n",
    "            'Name of the item currently running':i,\n",
    "            'Total no. of colours present in item':count_colours,\n",
    "            'timestamp': datetime.now()}\n",
    "        ID+=1\n",
    "        res = elastic.index(index=\"log-index\", id=ID, body=doc2)\n",
    "        elastic.indices.refresh(index=\"log-index\")\n",
    "        print(\"Total no. of colours present in item-\",i,\"is\",count_colours)\n",
    "        count_c=0        \n",
    "        for k in u_colors:\n",
    "            count_c+=1\n",
    "            sizes = unique_sizes(Region,channel,i,k)\n",
    "            count_sizes=len(sizes)\n",
    "            doc3 = {\n",
    "                'Name of the item currently running':i,\n",
    "                'Name of the colour currently running':k,\n",
    "                'Total no. of sizes present in color':count_sizes,\n",
    "                'timestamp': datetime.now()}\n",
    "            ID+=1\n",
    "            res = elastic.index(index=\"log-index\", id=ID, body=doc3)\n",
    "            elastic.indices.refresh(index=\"log-index\")\n",
    "            print(\"Total no. of sizes present in item-\",i,\"colour-\",k,\"is\",count_sizes)\n",
    "            count_s=0            \n",
    "            for j in sizes:\n",
    "                count_s+=1\n",
    "                temp=dimensions(Region,channel,i,k,j)\n",
    "                temp['INVOICEDATE'] = pd.to_datetime(temp['INVOICEDATE'])\n",
    "                temp['LINEAMOUNT'] = temp['LINEAMOUNT'].astype(float)\n",
    "                last_date=temp['INVOICEDATE'].iloc[-1]\n",
    "                Forecast_first_date = last_date + timedelta(1)\n",
    "                try:                    \n",
    "                    reg=np.full(shape=Forecast_days,fill_value=Region)\n",
    "                    chnl=np.full(shape=Forecast_days,fill_value=channel)\n",
    "                    product=np.full(shape=Forecast_days,fill_value=i)\n",
    "                    col=np.full(shape=Forecast_days,fill_value=k)\n",
    "                    dim=np.full(shape=Forecast_days,fill_value=j)                    \n",
    "                    d = {'Region':reg,'SalesChannel':chnl,'ITEMID':product,'Colour':col,'SIZE':dim}\n",
    "                    df_Forecasted_all = pd.DataFrame(d)\n",
    "                    df_Forecasted_all['UNIQ_ID'] = df_Forecasted_all[['ITEMID','Colour','SIZE']].apply(lambda x: '_'.join(x), axis = 1) \n",
    "                    df_Forecasted_all['UNIQ_ID'] =df_Forecasted_all['Region'].astype(str)+'_'+df_Forecasted_all['SalesChannel'].astype(str)+'_'+df_Forecasted_all['UNIQ_ID']\n",
    "                    combination=df_Forecasted_all['UNIQ_ID'].iloc[-1]                    \n",
    "                    Train=temp\n",
    "                    train = Train.iloc[:, 1:2].values                    \n",
    "                    scaler = MinMaxScaler()\n",
    "                    train_df = train.reshape(-1,1)\n",
    "                    scaler.fit(train_df)\n",
    "                    train_df = scaler.transform(train_df)                    \n",
    "                    def create_dataset(dataset, look_back):\n",
    "                        dataX, dataY = [], []\n",
    "                        for i in range(len(dataset)-look_back):\n",
    "                            a = dataset[i:(i + look_back), 0]\n",
    "                            dataX.append(a)\n",
    "                            dataY.append(dataset[i + look_back, 0])\n",
    "                        return np.array(dataX), np.array(dataY)                    \n",
    "                    time_steps = 90\n",
    "                    X_train, y_train = create_dataset(train_df, time_steps)                    \n",
    "                    X_train = np.reshape(X_train, (X_train.shape[0], time_steps, 1))                    \n",
    "                    print(\"Model running for Item-\",i,\"colour-\",k,\"size-\",j)                    \n",
    "                    model = Sequential()\n",
    "                    model.add(Conv1D(filters=256, kernel_size=2, activation='relu', input_shape=(90,1)))\n",
    "                    model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "                    model.add(MaxPooling1D(pool_size=2))\n",
    "                    model.add(Flatten())\n",
    "                    model.add(RepeatVector(30))\n",
    "                    model.add(LSTM(128, activation='relu'))\n",
    "                    model.add(Dense(100, activation='relu'))\n",
    "                    model.add(Dense(1))                    \n",
    "                    model.compile(metrics = ['accuracy'], optimizer = 'adam', loss = 'mean_squared_error')                    \n",
    "                    history = model.fit(X_train, y_train, epochs = 100, batch_size = 20, validation_split=.10)                    \n",
    "                    pred_list = []\n",
    "                    Last_set=train_df[-90:]\n",
    "                    batch = np.reshape(Last_set, (1, 90, 1))\n",
    "                    for z in range(Forecast_days):  \n",
    "                        pred = model.predict(batch)\n",
    "                        pred_list.append(pred[0,0]) \n",
    "                        pred = pred.reshape(1,1,1) \n",
    "                        batch = np.concatenate((batch[:,1:,:], pred), axis=1)\n",
    "                    pred_list_array=np.array(pred_list)\n",
    "                    pred_list_array=pred_list_array.reshape(-1, 1)\n",
    "                    pred_list_Unscaled = scaler.inverse_transform(pred_list_array)              \n",
    "                    df_Forecasted = pd.DataFrame(pred_list_Unscaled, columns=[\"FORECASTVALUE\"])\n",
    "                    forecast_dates=pd.date_range(Forecast_first_date, periods=Forecast_days)\n",
    "                    df_Forecasted_dates = pd.DataFrame(forecast_dates, columns=['DATE'])\n",
    "                    df_Forecasted_dates=df_Forecasted_dates.astype(str)                    \n",
    "                    temp_df = pd.concat([df_Forecasted_all,df_Forecasted_dates,df_Forecasted], axis = 1)                   \n",
    "                    cols = \"`,`\".join([str(i) for i in temp_df.columns.tolist()])\n",
    "                    for l,row in temp_df.iterrows():\n",
    "                        sql = \"INSERT INTO `TESTFORECAST_OUTPUT` (`\" +cols + \"`) VALUES (\" + \"%s,\"*(len(row)-1) + \"%s)\"\n",
    "                        cursor.execute(sql, tuple(row))\n",
    "                        connection.commit()\n",
    "                    doc4 = {\n",
    "                            'Name of the combination completed': combination,\n",
    "                            'No. of items remaining': count_items-count_i,\n",
    "                            'No. of colours remaining':count_colours-count_c,\n",
    "                            'No. of sizes remaining': count_sizes-count_s,\n",
    "                            'timestamp': datetime.now()\n",
    "                      \n",
    "                        }\n",
    "                    ID=ID+1\n",
    "                    res = elastic.index(index=\"log-index\", id=ID, body=doc4)\n",
    "                    res = elastic.get(index=\"log-index\", id=ID)\n",
    "                    print(res['_source'])\n",
    "                    elastic.indices.refresh(index=\"log-index\")\n",
    "                    print(\"Name of the combination completed\",df_Forecasted_all['UNIQ_ID'].iloc[-1])\n",
    "                    print(\"No. of items remaining\",count_items-count_i)\n",
    "                    print(\"No.of colours remaining in item-\",i,\"is\",count_colours-count_c)\n",
    "                    print(\"No. of sizes remaining in item-\",i,\"colour-\",k,\"is\",count_sizes-count_s)                 \n",
    "                except Exception as e:\n",
    "                    Error=str(e)\n",
    "                    print(Error)\n",
    "                    localtime = time.asctime( time.localtime(time.time()))\n",
    "                    combination=str(combination)\n",
    "                    localtime=str(localtime)\n",
    "                    sql = \"INSERT INTO TESTERROR_LOGS (UNIQ_ID,ERROR_DATE,ERROR) VALUES (%s, %s, %s)\"\n",
    "                    val = (combination,localtime,Error)\n",
    "                    cursor.execute(sql, val)\n",
    "                    connection.commit()\n",
    "                    print(\"Exception handled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=Forecast1(4,3,90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
